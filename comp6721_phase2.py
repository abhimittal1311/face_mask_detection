# -*- coding: utf-8 -*-
"""COMP6721_Phase2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-bNRukKDiFVwP4lZ-01kBoWOlben0zO-

Importing all necessary libraries
"""

import warnings
warnings.filterwarnings("ignore")
import os
import sklearn
import torchvision
import numpy
import torch
import matplotlib

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
import torch.optim as optim

from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader, Sampler, random_split
from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.metrics import ConfusionMatrixDisplay

from torchvision import datasets
from torchvision.transforms import transforms
from torch.optim import Adam
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay

"""Loading the Dataset and setting the transform values"""

from google.colab import drive
drive.mount('/content/drive')

datadir = "/content/drive/MyDrive/Comp6721_Dataset/Final_Dataset" 


trans = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.Resize((128)), 
    transforms.CenterCrop(60), transforms.ToTensor(), 
    transforms.Normalize(mean=[0.6569, 0.5629, 0.5234],std=[0.4778, 0.4677, 0.4861])
    ])


dataset_source = datasets.ImageFolder(datadir, transform=trans)

"""Building Base Convolutional Layer"""

class Basic_Cell_Conv_Net(nn.Module): 
    
    def __init__(self, in_channels, out_channels):
        super(Basic_Cell_Conv_Net, self).__init__()  
        self.conv_layer = nn.Conv2d(in_channels=in_channels, kernel_size=3, out_channels=out_channels, stride=1, padding=1)
        # Appling Batch Normalization.
        self.Batch_Normalization = nn.BatchNorm2d(num_features = out_channels)
        # applying LeakyReLU activation function.
        self.relu = nn.LeakyReLU(inplace=True)
        
    def forward(self, input):
        x_out = self.conv_layer(input)
        x_out = self.Batch_Normalization(x_out)
        x_out = self.relu(x_out)
        return x_out

"""Building the Model with Convolutional Layers"""

class Model_Convolutional_Network(nn.Module):
    def __init__(self, output_classes):
        super(Model_Convolutional_Network, self).__init__()

        # Input layer (3x32).
        self.Basic_Cell_Conv_input = Basic_Cell_Conv_Net(in_channels=3, out_channels=32)
        self.pool_Max_1 = nn.MaxPool2d(kernel_size=2)

        # hidden layer 1 (32x32)
        self.Basic_Cell_Conv_Net_hidden_1 = Basic_Cell_Conv_Net(in_channels=32, out_channels=32)
        self.pool_Max_2 = nn.MaxPool2d(kernel_size=2)
        
        # hidden layer 2 (32x128)
        self.Basic_Cell_Conv_Net_hidden_2 = Basic_Cell_Conv_Net(in_channels=32, out_channels=128)
        self.pool_Max_3 = nn.MaxPool2d(kernel_size=2)

        # hidden layer 3 (128 x 128)
        self.Basic_Cell_Conv_Net_hidden_3 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)
        
        self.pool_avg = nn.AvgPool2d(kernel_size=4)
        
    
        self.net = nn.Sequential(self.Basic_Cell_Conv_input, 
                                 self.pool_Max_1,
                                 
                                 self.Basic_Cell_Conv_Net_hidden_1,
                                 self.pool_Max_2,
                                 
                                 self.Basic_Cell_Conv_Net_hidden_2, 
                                  self.pool_Max_3,
                                 
                                self.Basic_Cell_Conv_Net_hidden_3,

                                 self.pool_avg
                                 )
        
        # build the linear neural network.
        self.fc_layer = nn.Sequential(
            nn.Dropout(p=0.1),
            nn.Linear(128, 1000),
            nn.ReLU(inplace=True),
            nn.Linear(1000, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(in_features=128, out_features=output_classes)
            )
        
    def forward(self, input):
        _x_ = self.net(input)
        _x_ = _x_.view(-1, 128) 
        _x_ = self.fc_layer(_x_)
        return _x_

"""Combining all the Layers and adding Dropout and Max Pooling """

class Model_Convolutional_Network(nn.Module):
    
    def __init__(self, output_classes):
        super(Model_Convolutional_Network, self).__init__()
        # Creating modelCNNs layers with max pooling among.
        self.Basic_Cell_Conv_Net_1 = Basic_Cell_Conv_Net(in_channels=3, out_channels=128)
        self.Basic_Cell_Conv_Net_2 = Basic_Cell_Conv_Net(in_channels=128, out_channels=32)
        self.Basic_Cell_Conv_Net_3 = Basic_Cell_Conv_Net(in_channels=32, out_channels=32)

        self.pool_Max_1 = nn.MaxPool2d(kernel_size=2)

        self.Basic_Cell_Conv_Net_4 = Basic_Cell_Conv_Net(in_channels=32, out_channels=64)
        self.Basic_Cell_Conv_Net_5 = Basic_Cell_Conv_Net(in_channels=64, out_channels=64)

        self.pool_Max_2 = nn.MaxPool2d(kernel_size=2)
        self.Basic_Cell_Conv_Net_6 = Basic_Cell_Conv_Net(in_channels=64, out_channels=128)
        self.Basic_Cell_Conv_Net_7 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)

        self.pool_Max_3 = nn.MaxPool2d(kernel_size=2)

        self.Basic_Cell_Conv_Net_8 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)
        self.Basic_Cell_Conv_Net_9 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)
        
        # Apply Average pooling before flatten.
        self.pool_avg = nn.AvgPool2d(kernel_size=4)
        
        # combining all created neural units into a Sequential layer in sequence.
        self.net = nn.Sequential(self.Basic_Cell_Conv_Net_1, 
                                 self.Basic_Cell_Conv_Net_2,
                                 self.Basic_Cell_Conv_Net_3,
                                 self.pool_Max_1,
                                 self.Basic_Cell_Conv_Net_4,
                                 self.Basic_Cell_Conv_Net_5, 
                                 self.pool_Max_2,
                                 self.Basic_Cell_Conv_Net_6, 
                                 self.Basic_Cell_Conv_Net_7, 
                                  self.pool_Max_3,
                                self.Basic_Cell_Conv_Net_8,
                                self.Basic_Cell_Conv_Net_9,
                                 self.pool_avg
                                 )
        
        # build the linear neural network.
        self.fc_layer = nn.Sequential(
            nn.Dropout(p=0.1),
            nn.Linear(128, 1000),
            nn.ReLU(inplace=True),
            nn.Linear(1000, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(in_features=128, out_features=output_classes)
            )
        
    def forward(self, input):
        _x_ = self.net(input)
        _x_ = _x_.view(-1, 128) 
        _x_ = self.fc_layer(_x_)
        return _x_

"""Training Parameters"""

batch_size = 96 
output_classes = 5
val_recall = []
val_f1 = []
val_accuracy = []
val_precision = []
Kfold_confusion_matrix = []

foldsperformance = {}
k_folds_No=10
splits=KFold(n_splits=k_folds_No,shuffle=True,random_state=42)

"""Applying 10-Fold Validation and Storing the Metrics"""

for fold, (train_index, validation_index) in enumerate(splits.split(np.arange(len(dataset_source)))):
    
    train_loss_ = []
    test_loss_ = []
    train_acc_ = []
    test_acc_ = []
    
    epoch = 1
    total_epochs = 10 
    
    performance_history = {'train_loss_': [], 'test_loss_': [],'train_acc_':[],'test_acc_':[]}
    print('Fold {}'.format(fold + 1))
    trainingSet = torch.utils.data.Subset(dataset_source, train_index)
    testingSet = torch.utils.data.Subset(dataset_source, validation_index)
    trainloader = torch.utils.data.DataLoader(dataset=trainingSet, batch_size=batch_size, shuffle=True)
    testloader = torch.utils.data.DataLoader(dataset=testingSet, batch_size=batch_size, shuffle=False)
    
    model = Model_Convolutional_Network(output_classes)
    optimizer = optim.Adam(model.parameters(), lr=0.001) 
    loss_criteria = nn.CrossEntropyLoss()
    
    while epoch <= total_epochs:
        loss_per_iteration, total_loops, correct =0.0, 0, 0 
        
        # Start the training phase.
        for i, (images, labels) in enumerate(trainloader):
            total_loops += 1
            images = Variable(images)
            labels = Variable(labels)
       
            All_correct_values = 0
            correct_values = 0

            model.train()
            optimizer.zero_grad()

            train_predictions = model(images)

            train_loss = loss_criteria(train_predictions, labels)

            train_loss.backward()
            optimizer.step()

            _, predicted = torch.max(train_predictions, 1)
            correct_values += (predicted == labels).sum()
            correct += correct_values.item()  
            
            loss_per_iteration += train_loss.item()
        
#         print("total_loops: ", total_loops)
#         print("len(trainingSet): ", len(trainingSet))
        
        train_acc_temp = (100 * correct / len(trainingSet))
        train_loss_temp = (loss_per_iteration / total_loops)

        train_loss_.append(loss_per_iteration / total_loops)
        train_acc_.append((100 * correct / len(trainingSet))) 
        
      
        performance_history['train_loss_'].append(train_loss_temp)
        performance_history['train_acc_'].append(train_acc_temp)
        
        # Start validation/testing phase.
        model.eval()
        with torch.no_grad():
            loss_per_iteration, total_loops, correct = 0.0, 0, 0 
            
            for i, (images, labels) in enumerate(testloader):
                total_loops += 1
                images = Variable(images)
                labels = Variable(labels)
                
                correct_values = 0
                test_predictions = model(images)
                test_loss = loss_criteria(test_predictions, labels)

                _, predicted = torch.max(test_predictions, 1)
                correct_values += (predicted == labels).sum()

                correct += correct_values.item() 
                loss_per_iteration +=  test_loss.item()
        
#         print("total_loops: ", total_loops)
#         print("len(testloader): ", len(testloader))
        
        test_acc_temp = (100 * correct / len(testingSet))
        test_loss_temp = loss_per_iteration / total_loops

        test_loss_.append(loss_per_iteration / total_loops)
        test_acc_.append((100 * correct / len(testingSet)))

        performance_history['test_loss_'].append(test_loss_temp)            
        performance_history['test_acc_'].append(test_acc_temp)
        
        print('Epoch {}, Training Loss: {:.2f}, Training Accuracy: {:.2f} %, ''Test Loss: {:.2f}, Testing Accuracy: {:.2f} %'.format
              (epoch, train_loss_temp, train_acc_temp, test_loss_temp, test_acc_temp))        
        epoch += 1
    # Store all training accuracies, losses, testing accurices, testing losses per fold.  
    foldsperformance['fold{}'.format(fold+1)] = performance_history 
    
    # performing the validation test to store precision, recall, accuracy, and confusion matrix per fold.
    if epoch == total_epochs + 1:
        y_test = []
        y_pred = []
        testloader = torch.utils.data.DataLoader(dataset=testingSet, shuffle=False)
        model.eval()
        with torch.no_grad():
            for i, (image, label) in enumerate(testloader):
                
                test_prediction = model(image)
                _, predicted_value = torch.max(test_prediction, 1)
                               
                predicted_value_ = predicted_value.tolist()[0]
                y_pred.append(predicted_value_)
                
                y_test_ = label.tolist()[0]
                y_test.append(y_test_)
                
        # Storing the accuracies, precisions, recall, and f1-measure for validation set.
        precision = precision_score(y_test, y_pred, average='macro')
        val_precision.append(precision)
            
        accuracy = accuracy_score(y_test, y_pred)
        val_accuracy.append(accuracy)
        
        f1 = f1_score(y_test, y_pred, average='macro')
        val_f1.append(f1)
        
        recall = recall_score(y_test, y_pred, average='macro')
        val_recall.append(recall)

"""Printing the Metrics"""

print(f"Averaged K-folds Performance during {total_epochs} epochs:")
print("Averaged Accuracy: ", np.array(val_accuracy).mean())
print("Averaged Precision: ", np.array(val_precision).mean())
print("Averaged Recall: ", np.array(val_recall).mean())
print("Averaged F1_score: ", np.array(val_f1).mean())

"""Storing the performance of each fold to plot the Performance Graph"""

import statistics
train_loss_each_epoch = []
test_loss_each_epoch = []
train_acc_each_epoch = []
test_acc_each_epoch = []

train_loss_avg_each_epoch = []
test_loss_avg_each_epoch = []
train_acc_avg_each_epoch = []
test_acc_avg_each_epoch = []

thershold = 0


for itera_ in range(total_epochs):
    for fold_ in foldsperformance:
        for measure in foldsperformance[fold_]:
            if thershold == itera_:
                # the following for train_loss_: 
                if measure is 'train_loss_':
                        train_loss_each_epoch.append(foldsperformance[fold_][measure][itera_])

                if measure is 'test_loss_':
                        test_loss_each_epoch.append(foldsperformance[fold_][measure][itera_])

                if measure is 'train_acc_':
                        train_acc_each_epoch.append(foldsperformance[fold_][measure][itera_])

                if measure is 'test_acc_':
                        test_acc_each_epoch.append(foldsperformance[fold_][measure][itera_])
    
    train_loss_avg_each_epoch.append(statistics.mean(train_loss_each_epoch))
    
    test_loss_avg_each_epoch.append(statistics.mean(test_loss_each_epoch))

    train_acc_avg_each_epoch.append(statistics.mean(train_acc_each_epoch))

    test_acc_avg_each_epoch.append(statistics.mean(test_acc_each_epoch))
    
    train_loss_each_epoch = []
    test_loss_each_epoch = []
    train_acc_each_epoch = []
    test_acc_each_epoch = []
    
    thershold+=1

epochs = range(1, total_epochs+1)
plt.rcParams["figure.figsize"] = (12,10)
plt.plot(epochs, train_loss_avg_each_epoch, 'g', label='Training loss')
plt.plot(epochs, test_loss_avg_each_epoch, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(1, total_epochs+1)
plt.rcParams["figure.figsize"] = (12,10)
plt.plot(epochs,train_acc_avg_each_epoch, 'g', label='Training accuracy')
plt.plot(epochs, test_acc_avg_each_epoch, 'b', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy %')
plt.legend()
plt.show()

