# -*- coding: utf-8 -*-
"""COMP6721_project_phase_1_2_noKfold_biastesting_balanced_data_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/172bQVlxgYQESqmpM_LtRkxsewQGhI1xp
"""

"cell No. 1"
# loading the required libraries.

import warnings
warnings.filterwarnings("ignore")
import os
import sklearn
import torchvision
import numpy
import torch
import matplotlib

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
import torch.optim as optim

from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader, Sampler, random_split
from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.metrics import ConfusionMatrixDisplay

from torchvision import datasets
from torchvision.transforms import transforms
from torch.optim import Adam
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay

from google.colab import drive
drive.mount('/content/drive')

"cell No. 2"

# print and verify the installed libraries versions

print("sklearn version: ", sklearn.__version__) # 1.0.2
print("torch version: ", torch.__version__) # 1.10.0+cpu
print("numpy version: ", numpy.__version__) # 1.21.5
print("torchvision version: ", torchvision.__version__) # 0.10.0
print("matplotlib version:", matplotlib.__version__) # 3.5.1

"cell No. 3"

# The following will create a basic convolutional neural network unit to build the model.
class Basic_Cell_Conv_Net(nn.Module): # Basic_Cell_Conv_Net
    
    def __init__(self, in_channels, out_channels):
        super(Basic_Cell_Conv_Net, self).__init__()  #Basic_Cell_Conv_Net
        # creating basic Conv2d neural unit
        self.conv_layer = nn.Conv2d(in_channels=in_channels, kernel_size=3, out_channels=out_channels, stride=1, padding=1)
        # Appling Batch Normalization.
        self.Batch_Normalization = nn.BatchNorm2d(num_features = out_channels)
        # applying LeakyReLU activation function.
        self.relu = nn.LeakyReLU(inplace=True)
        
    def forward(self, input):
        x_out = self.conv_layer(input)
        x_out = self.Batch_Normalization(x_out)
        x_out = self.relu(x_out)
        return x_out

"cell No. 4"
# activating model phase 1 convolutional neural networks (CNNs) and linear neural networks (LNNs).

class Model_Convolutional_Network(nn.Module):
    def __init__(self, output_classes):
        super(Model_Convolutional_Network, self).__init__()

        # Input layer (3x32).
        self.Basic_Cell_Conv_input = Basic_Cell_Conv_Net(in_channels=3, out_channels=32)
        self.pool_Max_1 = nn.MaxPool2d(kernel_size=2)

        # hidden layer 1 (32x32)
        self.Basic_Cell_Conv_Net_hidden_1 = Basic_Cell_Conv_Net(in_channels=32, out_channels=32)
        self.pool_Max_2 = nn.MaxPool2d(kernel_size=2)
        
        # hidden layer 2 (32x128)
        self.Basic_Cell_Conv_Net_hidden_2 = Basic_Cell_Conv_Net(in_channels=32, out_channels=128)
        self.pool_Max_3 = nn.MaxPool2d(kernel_size=2)

        # hidden layer 3 (128 x 128)
        self.Basic_Cell_Conv_Net_hidden_3 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)
        
        # Apply Average pooling before flatten.
        self.pool_avg = nn.AvgPool2d(kernel_size=4)
        
        # combining all created CNNs into a Sequential layers in sequence.
        self.net = nn.Sequential(self.Basic_Cell_Conv_input, 
                                 self.pool_Max_1,
                                 
                                 self.Basic_Cell_Conv_Net_hidden_1,
                                 self.pool_Max_2,
                                 
                                 self.Basic_Cell_Conv_Net_hidden_2, 
                                  self.pool_Max_3,
                                 
                                self.Basic_Cell_Conv_Net_hidden_3,

                                 self.pool_avg
                                 )
        
        # build the linear neural network.
        self.fc_layer = nn.Sequential(
            nn.Dropout(p=0.1),
            nn.Linear(128, 1000),
            nn.ReLU(inplace=True),
            nn.Linear(1000, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(in_features=128, out_features=output_classes)
            )
        
    def forward(self, input):
        _x_ = self.net(input)
        _x_ = _x_.view(-1, 128) # reshape the output of the conv neural network before fed into the linear network
        _x_ = self.fc_layer(_x_)
        return _x_

"cell No. 5"

# activating model phase 2 convolutional neural networks (CNNs) and linear neural networks (LNNs).

class Model_Convolutional_Network(nn.Module):
    
    def __init__(self, output_classes):
        super(Model_Convolutional_Network, self).__init__()
        # Creating modelCNNs layers with max pooling among.
        self.Basic_Cell_Conv_Net_1 = Basic_Cell_Conv_Net(in_channels=3, out_channels=128)
        self.Basic_Cell_Conv_Net_2 = Basic_Cell_Conv_Net(in_channels=128, out_channels=32)
        self.Basic_Cell_Conv_Net_3 = Basic_Cell_Conv_Net(in_channels=32, out_channels=32)

        self.pool_Max_1 = nn.MaxPool2d(kernel_size=2)

        self.Basic_Cell_Conv_Net_4 = Basic_Cell_Conv_Net(in_channels=32, out_channels=64)
        self.Basic_Cell_Conv_Net_5 = Basic_Cell_Conv_Net(in_channels=64, out_channels=64)

        self.pool_Max_2 = nn.MaxPool2d(kernel_size=2)
        self.Basic_Cell_Conv_Net_6 = Basic_Cell_Conv_Net(in_channels=64, out_channels=128)
        self.Basic_Cell_Conv_Net_7 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)

        self.pool_Max_3 = nn.MaxPool2d(kernel_size=2)

        self.Basic_Cell_Conv_Net_8 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)
        self.Basic_Cell_Conv_Net_9 = Basic_Cell_Conv_Net(in_channels=128, out_channels=128)
        
        # Apply Average pooling before flatten.
        self.pool_avg = nn.AvgPool2d(kernel_size=4)
        
        # combining all created neural units into a Sequential layer in sequence.
        self.net = nn.Sequential(self.Basic_Cell_Conv_Net_1, 
                                 self.Basic_Cell_Conv_Net_2,
                                 self.Basic_Cell_Conv_Net_3,
                                 self.pool_Max_1,
                                 self.Basic_Cell_Conv_Net_4,
                                 self.Basic_Cell_Conv_Net_5, 
                                 self.pool_Max_2,
                                 self.Basic_Cell_Conv_Net_6, 
                                 self.Basic_Cell_Conv_Net_7, 
                                  self.pool_Max_3,
                                self.Basic_Cell_Conv_Net_8,
                                self.Basic_Cell_Conv_Net_9,
                                 self.pool_avg
                                 )
        
        # build the linear neural network.
        self.fc_layer = nn.Sequential(
            nn.Dropout(p=0.1),
            nn.Linear(128, 1000),
            nn.ReLU(inplace=True),
            nn.Linear(1000, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(in_features=128, out_features=output_classes)
            )
        
    def forward(self, input):
        _x_ = self.net(input)
        _x_ = _x_.view(-1, 128) # reshape the output of the conv neural network before fed into the linear network
        _x_ = self.fc_layer(_x_)
        return _x_

"cell No. 6"
# loading a trained model on balanced dataset.
output_classes = 5

PATH = "/content/drive/MyDrive/COMP6721_PROJECT_Gr_AK_17_phase_2 (1)/COMP6721_PROJECT_Gr_AK_17_phase_2/balanced_full_data_set_model.pth"

model = Model_Convolutional_Network(output_classes)
model.load_state_dict(torch.load(PATH),strict=False)

"cell No. 7"

# define and set the global variables.

num_epochs = 100 # set the number of iterations for the training phase.
output_classes = 5 # set the number of output classes to be predicited for each data sample image.
learning_rate = 0.001 # set the learning rate for the training phase.

# Set the main directory path of the images dataset or repository used in training and testing the model.
# datadir = './full_data_set' # non balanaced
datadir = '/content/drive/MyDrive/COMP6721_PROJECT_Gr_AK_17_phase_2 (1)/COMP6721_PROJECT_Gr_AK_17_phase_2/full_data_set'
# set the transforms of pytorch with the mean and standard deviation of the dataset images.
trans = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.Resize((128)), # get a patch of the input image in a square shape(h:32 x w:32)
    transforms.CenterCrop(60), transforms.ToTensor(), # Scale the image uniformly (maintain the image's aspect ratio)
    transforms.Normalize(mean=[0.6569, 0.5629, 0.5234],std=[0.4778, 0.4677, 0.4861])
    ])

# set the full path for the hosting directory of the images repository.
dataset_source = datasets.ImageFolder(datadir, transform=trans)

# set the count range of the dataset images to compute the training and testing split from the full dataset.
indexes_values = list(range(len(dataset_source)))

# training_testing_split = int(np.floor(1 * len(dataset_source))) # set the train split.

np.random.shuffle(indexes_values) # shuffling the indices values to balance the categories of the images used in training and testing phases. 

# Set the training and testing indices from the main dataset.

train_indexes = indexes_values

# Set the training and testing samples according to train_indexes and testing_indexes
training_subset = SubsetRandomSampler(train_indexes)
# testing_subset = SubsetRandomSampler(testing_indexes)

# loading the training and testing samples using torch library.
trainloader = torch.utils.data.DataLoader(dataset_source, sampler=training_subset, batch_size=50, num_workers=1)
# testloader = torch.utils.data.DataLoader(dataset_source, sampler=testing_subset, num_workers=1)

"cell No. 8"

# "start the training phase"

model = Model_Convolutional_Network(output_classes)
criterion_fun = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)

total_step = len(trainloader)
training_y_pred= []
training_set_targets_ = []
for epoch in range(num_epochs):
    for i, (_images_, labels_) in enumerate(trainloader):
        training_set_targets_.append(labels_[0])
        
        _images_ = Variable(_images_)
        labels_ = Variable(labels_)
        
        resutls = model(_images_)
        loss = criterion_fun(resutls, labels_)
     
        # Backprop and optimisation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Training accuracy
        training_total = labels_.size(0)
        
        _, predicted = torch.max(resutls.data, 1)
        training_y_pred.append(predicted.tolist()[0])
        
        correct = (predicted == labels_).sum().item()
        if (i + 1) % total_step == 0:
            print('Epoch [{}/{}], Step [{}/{}], TrainLoss: {:.2f}, Train Accuracy: {:.2f}%'
            .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), (correct / training_total) * 100))

"cell No. 9"
# saving the trained model

PATH = "/content/drive/MyDrive/COMP6721_PROJECT_Gr_AK_17_phase_2 (1)/COMP6721_PROJECT_Gr_AK_17_phase_2/balanced_full_data_set_model.pth"
# Save
torch.save(model.state_dict(), PATH)

"cell No. 10"
# start the testing phase on the black people dataset, then validating Testing accuracy, precision, recall and F1-measure table

datadir_test_black = '/content/drive/MyDrive/Comp6721_part2/Ai/full_data_set/classified-20220622T173841Z-001/classified/black'
trans = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.Resize((128)), # get a patch of the input image in a square shape(h:32 x w:32)
    transforms.CenterCrop(60), transforms.ToTensor(), # Scale the image uniformly (maintain the image's aspect ratio)
    transforms.Normalize(mean=[0.6569, 0.5629, 0.5234],std=[0.4778, 0.4677, 0.4861])
    ])

black_dataset_source_test = datasets.ImageFolder(datadir_test_black, transform=trans)

black_indexes_values_ = list(range(len(black_dataset_source_test)))
np.random.shuffle(black_indexes_values_)

black_test_indexes = black_indexes_values_
black_test_subset_ = SubsetRandomSampler(black_test_indexes)
black_testloader = torch.utils.data.DataLoader(black_dataset_source_test, sampler=black_test_subset_, num_workers=1)

#### bias testing####

black_y_pred = []
black_test_set_targets_ = []

model.eval()
with torch.no_grad():
    black_correct_hits = 0
    black_total_processed = 0
    
    for ii, (images__, labels__) in enumerate(black_testloader):
        black_test_set_targets_.append(labels__[0])
        
        black_test_outputs = model(images__)
        
        _, black_predicted = torch.max(black_test_outputs.data, 1)
        
        black_y_pred.append(black_predicted.tolist()[0])
        
        black_total_processed += labels__.size(0)
        
        black_correct_hits += (black_predicted == labels__).sum().item()
        
# validating Testing accuracy, precision, recall and F1-measure table

black_accuracy = accuracy_score(black_test_set_targets_, black_y_pred) 
black_precision = precision_score(black_test_set_targets_, black_y_pred, average='macro')
black_recall = recall_score(black_test_set_targets_, black_y_pred, average='macro')
black_f1 = f1_score(black_test_set_targets_, black_y_pred, average='macro')
black_c_matrix = confusion_matrix(black_test_set_targets_, black_y_pred)
print("---------------------------")
print("Testing / Evaluation stage:-")
print("---------------------------")
print("testing accuracy : ", black_accuracy)
print("testing precision : ", black_precision)
print("testing recall : ", black_recall)
print("testing f1_score : ", black_f1)
print()
print()
print("---------------------------")
print("Testing / Classification report results:-")
print("---------------------------")

# printing the dataset categories and assigned indexes
black_category_classes = dict((v,k) for k,v in black_dataset_source_test.class_to_idx.items())
for i in range (len(black_category_classes)):
    print(f"{i}:", black_category_classes[i])

# Print the classification_report
print("---------------------------")
print(classification_report(black_test_set_targets_, black_y_pred))
print("---------------------------")

print("---Testing / Confusion Matrix---:-")
# printing the dataset categories and assigned indexes
black_category_classes = dict((v,k) for k,v in black_dataset_source_test.class_to_idx.items())
for i in range (len(black_category_classes)):
    print(f"{i}:", black_category_classes[i])

# Showing the Confusion Matrix of the testing Phase
black_confusion_m_test = confusion_matrix(black_test_set_targets_, black_y_pred)
ConfusionMatrixDisplay(black_confusion_m_test, display_labels=[0, 1, 2, 3, 4]).plot()
plt.show()

"cell No. 11"
# start the testing phase on the white people dataset, then validating Testing accuracy, precision, recall and F1-measure table

datadir_test_white = '/content/drive/MyDrive/Comp6721_part2/Ai/full_data_set/classified-20220622T173841Z-001/classified/White'
trans = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.Resize((128)), # get a patch of the input image in a square shape(h:32 x w:32)
    transforms.CenterCrop(60), transforms.ToTensor(), # Scale the image uniformly (maintain the image's aspect ratio)
    transforms.Normalize(mean=[0.6569, 0.5629, 0.5234],std=[0.4778, 0.4677, 0.4861])
    ])

white_dataset_source_test = datasets.ImageFolder(datadir_test_white, transform=trans)

white_indexes_values_ = list(range(len(white_dataset_source_test)))
np.random.shuffle(white_indexes_values_)

# print(len(white_indexes_values_))
white_test_indexes = white_indexes_values_

white_test_subset_ = SubsetRandomSampler(white_test_indexes)
white_testloader = torch.utils.data.DataLoader(white_dataset_source_test, sampler=white_test_subset_, num_workers=1)


#### bias testing####

white_y_pred = []
white_test_set_targets_ = []

model.eval()
with torch.no_grad():
    white_correct_hits = 0
    white_total_processed = 0
    
    for ii, (images__, labels__) in enumerate(white_testloader):
        white_test_set_targets_.append(labels__[0])
        
        white_test_outputs = model(images__)
        
        _, white_predicted = torch.max(white_test_outputs.data, 1)
        
        white_y_pred.append(white_predicted.tolist()[0])
        
        white_total_processed += labels__.size(0)
        
        white_correct_hits += (white_predicted == labels__).sum().item()

# validating Testing accuracy, precision, recall and F1-measure table

white_accuracy = accuracy_score(white_test_set_targets_, white_y_pred) 
white_precision = precision_score(white_test_set_targets_, white_y_pred, average='macro')
white_recall = recall_score(white_test_set_targets_, white_y_pred, average='macro')
white_f1 = f1_score(white_test_set_targets_, white_y_pred, average='macro')
white_c_matrix = confusion_matrix(white_test_set_targets_, white_y_pred)
print("---------------------------")
print("Testing / Evaluation stage:-")
print("---------------------------")
print("testing accuracy : ", white_accuracy)
print("testing precision : ", white_precision)
print("testing recall : ", white_recall)
print("testing f1_score : ", white_f1)
print()
print()
print("---------------------------")
print("Testing / Classification report results:-")
print("---------------------------")

# printing the dataset categories and assigned indexes
white_category_classes = dict((v,k) for k,v in white_dataset_source_test.class_to_idx.items())
for i in range (len(white_category_classes)):
    print(f"{i}:", white_category_classes[i])

# Print the classification_report
print("---------------------------")
print(classification_report(white_test_set_targets_, white_y_pred))
print("---------------------------")

print("---Testing / Confusion Matrix---:-")
# printing the dataset categories and assigned indexes
white_category_classes = dict((v,k) for k,v in white_dataset_source_test.class_to_idx.items())
for i in range (len(white_category_classes)):
    print(f"{i}:", white_category_classes[i])

# Showing the Confusion Matrix of the testing Phase
white_confusion_m_test = confusion_matrix(white_test_set_targets_, white_y_pred)
ConfusionMatrixDisplay(white_confusion_m_test, display_labels=[0, 1, 2, 3, 4]).plot()
plt.show()

"cell No. 12"
# start the testing phase on the male people dataset, then validating Testing accuracy, precision, recall and F1-measure table

datadir_test_male = '/content/drive/MyDrive/Comp6721_part2/Ai/full_data_set/classified-20220622T173841Z-001/P2_Data/Male'
trans = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.Resize((128)), # get a patch of the input image in a square shape(h:32 x w:32)
    transforms.CenterCrop(60), transforms.ToTensor(), # Scale the image uniformly (maintain the image's aspect ratio)
    transforms.Normalize(mean=[0.6569, 0.5629, 0.5234],std=[0.4778, 0.4677, 0.4861])
    ])

male_dataset_source_test = datasets.ImageFolder(datadir_test_male, transform=trans)

male_indexes_values_ = list(range(len(male_dataset_source_test)))
np.random.shuffle(male_indexes_values_)

# print(len(male_indexes_values_))
male_test_indexes = male_indexes_values_

male_test_subset_ = SubsetRandomSampler(male_test_indexes)
male_testloader = torch.utils.data.DataLoader(male_dataset_source_test, sampler=male_test_subset_, num_workers=1)


#### bias testing####

male_y_pred = []
male_test_set_targets_ = []

model.eval()
with torch.no_grad():
    male_correct_hits = 0
    male_total_processed = 0
    
    for ii, (images__, labels__) in enumerate(male_testloader):
        male_test_set_targets_.append(labels__[0])
        
        male_test_outputs = model(images__)
        
        _, male_predicted = torch.max(male_test_outputs.data, 1)
        
        male_y_pred.append(male_predicted.tolist()[0])
        
        male_total_processed += labels__.size(0)
        
        male_correct_hits += (male_predicted == labels__).sum().item()

# validating Testing accuracy, precision, recall and F1-measure table

male_accuracy = accuracy_score(male_test_set_targets_, male_y_pred) 
male_precision = precision_score(male_test_set_targets_, male_y_pred, average='macro')
male_recall = recall_score(male_test_set_targets_, male_y_pred, average='macro')
male_f1 = f1_score(male_test_set_targets_, male_y_pred, average='macro')
male_c_matrix = confusion_matrix(male_test_set_targets_, male_y_pred)
print("---------------------------")
print("Testing / Evaluation stage:-")
print("---------------------------")
print("testing accuracy : ", male_accuracy)
print("testing precision : ", male_precision)
print("testing recall : ", male_recall)
print("testing f1_score : ", male_f1)
print()
print()
print("---------------------------")
print("Testing / Classification report results:-")
print("---------------------------")

# printing the dataset categories and assigned indexes
male_category_classes = dict((v,k) for k,v in male_dataset_source_test.class_to_idx.items())
for i in range (len(male_category_classes)):
    print(f"{i}:", male_category_classes[i])

# Print the classification_report
print("---------------------------")
print(classification_report(male_test_set_targets_, male_y_pred))
print("---------------------------")

print("---Testing / Confusion Matrix---:-")
# printing the dataset categories and assigned indexes
male_category_classes = dict((v,k) for k,v in male_dataset_source_test.class_to_idx.items())
for i in range (len(male_category_classes)):
    print(f"{i}:", male_category_classes[i])

# Showing the Confusion Matrix of the testing Phase
male_confusion_m_test = confusion_matrix(male_test_set_targets_, male_y_pred)
ConfusionMatrixDisplay(male_confusion_m_test, display_labels=[0, 1, 2, 3, 4]).plot()
plt.show()

"cell No. 13"
# start the testing phase on the female people dataset, then validating Testing accuracy, precision, recall and F1-measure table

datadir_test_female = '/content/drive/MyDrive/Comp6721_part2/Ai/full_data_set/classified-20220622T173841Z-001/P2_Data/Female'
trans = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.Resize((128)), # get a patch of the input image in a square shape(h:32 x w:32)
    transforms.CenterCrop(60), transforms.ToTensor(), # Scale the image uniformly (maintain the image's aspect ratio)
    transforms.Normalize(mean=[0.6569, 0.5629, 0.5234],std=[0.4778, 0.4677, 0.4861])
    ])

female_dataset_source_test = datasets.ImageFolder(datadir_test_female, transform=trans)

female_indexes_values_ = list(range(len(female_dataset_source_test)))
np.random.shuffle(female_indexes_values_)
female_test_indexes = female_indexes_values_

female_test_subset_ = SubsetRandomSampler(female_test_indexes)
female_testloader = torch.utils.data.DataLoader(female_dataset_source_test, sampler=female_test_subset_, num_workers=1)

#### bias testing####

female_y_pred = []
female_test_set_targets_ = []

model.eval()
with torch.no_grad():
    female_correct_hits = 0
    female_total_processed = 0
    
    for ii, (images__, labels__) in enumerate(female_testloader):
        female_test_set_targets_.append(labels__[0])
        
        female_test_outputs = model(images__)
        
        _, female_predicted = torch.max(female_test_outputs.data, 1)
        
        female_y_pred.append(female_predicted.tolist()[0])
        
        female_total_processed += labels__.size(0)
        
        female_correct_hits += (female_predicted == labels__).sum().item()

# validating Testing accuracy, precision, recall and F1-measure table

female_accuracy = accuracy_score(female_test_set_targets_, female_y_pred) 
female_precision = precision_score(female_test_set_targets_, female_y_pred, average='macro')
female_recall = recall_score(female_test_set_targets_, female_y_pred, average='macro')
female_f1 = f1_score(female_test_set_targets_, female_y_pred, average='macro')
female_c_matrix = confusion_matrix(female_test_set_targets_, female_y_pred)
print("---------------------------")
print("Testing / Evaluation stage:-")
print("---------------------------")
print("testing accuracy : ", female_accuracy)
print("testing precision : ", female_precision)
print("testing recall : ", female_recall)
print("testing f1_score : ", female_f1)
print()
print()
print("---------------------------")
print("Testing / Classification report results:-")
print("---------------------------")

# printing the dataset categories and assigned indexes
female_category_classes = dict((v,k) for k,v in female_dataset_source_test.class_to_idx.items())
for i in range (len(female_category_classes)):
    print(f"{i}:", female_category_classes[i])

# Print the classification_report
print("---------------------------")
print(classification_report(female_test_set_targets_, female_y_pred))
print("---------------------------")

print("---Testing / Confusion Matrix---:-")
# printing the dataset categories and assigned indexes
female_category_classes = dict((v,k) for k,v in female_dataset_source_test.class_to_idx.items())
for i in range (len(female_category_classes)):
    print(f"{i}:", female_category_classes[i])

# Showing the Confusion Matrix of the testing Phase
female_confusion_m_test = confusion_matrix(female_test_set_targets_, female_y_pred)
ConfusionMatrixDisplay(female_confusion_m_test, display_labels=[0, 1, 2, 3, 4]).plot()
plt.show()

